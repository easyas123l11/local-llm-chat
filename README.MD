# Local LLM Chat App

A local AI chat app built with:

- React + TypeScript frontend
- Node.js backend
- Ollama local LLM runtime

## Requirements

- Node.js
- Ollama installed

## Setup

### 1. Install Ollama

https://ollama.com/download

Pull model:

ollama pull llama3.2

---

### 2. Start backend

cd backend
npm install
npm start

---

### 3. Start frontend

cd frontend
npm install
npm run dev

Open browser to:

http://localhost:5173

## Features

- Local AI chat
- Streaming token responses
- React UI